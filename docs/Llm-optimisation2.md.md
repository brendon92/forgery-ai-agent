# Zaawansowana rewizja i uzupe≈Çnienie raportu

**Wyeksportowane jako Markdown ‚Äî zawiera ocenƒô aktualno≈õci, rozszerzenie bibliografii (wielu ≈∫r√≥de≈Ç), warianty rozwiƒÖza≈Ñ implementacyjnych dla z≈Ço≈ºonych agent√≥w AI oraz pseudokod.**  
≈πr√≥d≈Ço pierwotnego materia≈Çu: przes≈Çany raport ‚ÄûLlm-optimisation.md.md‚Äù.

---

## 1. Streszczenie (Executive summary)

- Tw√≥j raport trafnie identyfikuje obecne high-leverage praktyki: XML/znaczniki w promptach, prompt caching, semantyczne chunking, GraphRAG, DSPy / ‚Äûprogramming-not-prompting‚Äù, oraz problem ‚Äûlost-in-the-middle‚Äù.
- Od czasu publikacji czƒô≈õci materia≈Ç√≥w (2023‚Äì2025) te obszary potwierdzi≈Çy swojƒÖ przydatno≈õƒá, ale: **szczeg√≥≈Çy ekonomiki prompt caching, rekomendacje dotyczƒÖce CoT i rekomendacje formatu (Markdown vs JSON)** wymagajƒÖ doprecyzowania ‚Äî istniejƒÖ szczeg√≥≈Çowe warunki i trade-offs, kt√≥re trzeba rozwa≈ºyƒá przy wdro≈ºeniu. (patrz sekcje walidacji i rekomendacje).

Poni≈ºej: (1) walidacja i korekta twierdze≈Ñ z Twojego raportu, (2) rozbudowane rozwiƒÖzania architektoniczne, (3) praktyczne pseudokody implementacji agent√≥w, (4) obszerny zbi√≥r ≈∫r√≥de≈Ç/link√≥w.

---

## 2. Walidacja punkt po punkcie (co jest aktualne, co wymaga doprecyzowania)

### 2.1 XML tags / System prompts / separacja r√≥l

**Twierdzenie z raportu:** XML Tags pomagajƒÖ separowaƒá instrukcje od danych i sƒÖ rekomendowane (Anthropic).  
**Weryfikacja:** Potwierdzone ‚Äî dokumentacja Anthropic (Claude) explicite promuje u≈ºycie tag√≥w XML/`<instructions>` i struktur dla lepszej separacji i odporno≈õci na prompt injection. Jednak nie jest to uniwersalny ‚Äûstandard‚Äù ‚Äî r√≥≈ºne modele/serwisy (OpenAI, Azure, Bedrock) majƒÖ w≈Çasne rekomendacje; XML jest jednak praktycznym, sprawdzonym wzorcem.

**Zalecenie uzupe≈ÇniajƒÖce:** Dodaj warstwƒô walidacji wej≈õcia po stronie aplikacji (pre-sanitize), metadane (`source`, `date`, `trust_score`) i korzystaj z wyra≈∫nego `role:` (system, user, tool) + XML/Markdown delimiters. (bez polegania tylko na tagach).

---

### 2.2 Prompt Caching (koszty i latencja)

**Twierdzenie z raportu:** Prompt caching mo≈ºe redukowaƒá koszty wej≈õciowe nawet do ~90% i znaczƒÖco obni≈ºyƒá latencjƒô.  
**Weryfikacja:** Potwierdzone ‚Äî dostawcy (Anthropic, OpenAI, AWS Bedrock, Azure) oferujƒÖ mechanizmy prompt caching; efekty oszczƒôdno≈õci r√≥≈ºniƒÖ siƒô w zale≈ºno≈õci od: modelu, polityki cenowej (cache write vs read cost), dopasowania cache (exact/partial) oraz scenariusza u≈ºycia. OpenAI oficjalnie opisuje ~50% zni≈ºkƒô na cache hit dla du≈ºych prefiks√≥w; Anthropic publikowane przypadki dajƒÖ wy≈ºsze oszczƒôdno≈õci przy innych cenach zapisu/odczytu.

**Uwaga praktyczna:** Nie wszystkie zapytania bƒôdƒÖ cache-hit; cache wymaga zarzƒÖdzania wersjami kontekstu (schema versioning) i polityk zachowywania (privacy). Je≈õli Tw√≥j system czƒôsto zmienia prefiks (system prompt + knowledge), zyski sƒÖ mniejsze.

---

### 2.3 Markdown vs JSON jako format wej≈õciowy

**Twierdzenie z raportu:** Markdown jest bardziej token-efektywny (30‚Äì40% mniej token√≥w vs JSON).  
**Weryfikacja:** Og√≥lny wniosek ‚Äî _tak_, wolniejszy sk≈Çadniowy narzut JSON (cudzys≈Çowy, nawiasy) zwykle kosztuje wiƒôcej token√≥w ni≈º czysty tekst/markdown, ale konkretna liczba procentowa zale≈ºy od danych i struktury. R√≥≈ºne ≈∫r√≥d≈Ça m√≥wiƒÖ o ~11‚Äì30% oszczƒôdno≈õci w praktyce; eksperymenty pokazujƒÖ te≈º, ≈ºe HTML/TSV/TOON mogƒÖ byƒá jeszcze bardziej oszczƒôdne w okre≈õlonych przypadkach. Nie ma ‚Äûuniwersalnego 30‚Äì40%‚Äù ‚Äî zale≈ºy od formatu danych i ich typ√≥w.

**Zalecenie:** Dla dokumentacji i czytelno≈õci: Markdown. Dla wyj≈õcia maszynowego: _zmuszaƒá_ model do zwrotu parsowalnego JSON (JSON Schema / `response_format={"type":"json_object"}`) lub u≈ºyƒá token-efektywnych notacji (CSV/TSV/TOON) tam, gdzie to mo≈ºliwe. Przetestuj konkretne workflows (A/B) mierzƒÖc tokeny i skuteczno≈õƒá.

---

### 2.4 Chain-of-Thought (CoT) i modele ‚Äûreasoning‚Äù (o1/o3)

**Twierdzenie z raportu:** W nowych reasoning modelach (o1/o3) nie nale≈ºy explicitnie wymuszaƒá CoT ‚Äî modele generujƒÖ ‚Äûukryte‚Äù reasoning tokens.  
**Weryfikacja:** Czƒô≈õciowo prawda. Dla niekt√≥rych najnowszych modeli producent zaleca uproszczone instrukcje i unikanie jawnego CoT, bo mo≈ºe kosztowaƒá i czasem pogarszaƒá wynik. Jednak to **zale≈ºy** od zadania (algorytmiczne rozumowanie vs ekstrakcja fakt√≥w). Testuj na docelowym modelu; nie ma uniwersalnego zakazu CoT, lecz trzeba rozwa≈ºyƒá koszt/benefit. (≈∫r√≥d≈Ça: rekomendacje OpenAI dla o1, opisy praktyk).

**Zalecenie:** W pipeline CI/CD uruchamiaj testy por√≥wnujƒÖce: (A) prosty prompt (constraints + goal) vs (B) prompt z CoT; ewaluuj accuracy + token cost + latency.

---

### 2.5 GraphRAG vs Vector RAG

**Twierdzenie z raportu:** GraphRAG znaczƒÖco poprawia odpowiedzi przy ‚Äûglobalnych pytaniach‚Äù i analizie relacji.  
**Weryfikacja:** Potwierdzone przez Microsoft Research i publikacje GraphRAG: po≈ÇƒÖczenie grafu wiedzy z RAG poprawia zdolno≈õƒá ≈ÇƒÖczenia rozrzuconych informacji i odpowiadania na pytania wymagajƒÖce rozumienia relacji. Jednak koszt preprocesingu i przechowywania grafu jest wiƒôkszy; potrzebne sƒÖ te≈º metody scalania (graph summarization) i mechanizmy filtrowania.

**Zalecenie:** Hybrydowy stack: Vector DB + lightweight knowledge graph (enkapsulujƒÖcy encje i relacje) + RAG z krokiem ‚Äûgraph traversal‚Äù generujƒÖcym fragmenty do finalnego promptu. Mierz poprawƒô jako≈õci vs kosztu.

---

## 3. Aktualno≈õƒá kluczowych wniosk√≥w (czy wnioski z raportu sƒÖ nadal wa≈ºne?)

- **Tak**: XML tags, prompt caching, semantyczny chunking, GraphRAG, DSPy-style modular prompt programming oraz U-curve (Lost in the Middle) pozostajƒÖ istotne i praktycznie u≈ºywane.
- **Doprecyzowaƒá**: liczby procentowe oszczƒôdno≈õci token√≥w (Markdown vs JSON), wysoko≈õƒá oszczƒôdno≈õci w prompt caching (r√≥≈ºni siƒô u dostawc√≥w), oraz zalecenie ‚Äûzupe≈Çnego unikania CoT‚Äù dla reasoning model√≥w ‚Äî to zale≈ºy od modelu i zadania.

---

## 4. Rozszerzone rekomendacje techniczne (konkretne, best-practice)

### 4.1 Architektura RAG dla z≈Ço≈ºonych agent√≥w (zr√≥≈ºnicowane opcje)

1. **Lightweight Hybrid RAG (production friendly)**
    
    - Vector DB (for semantic retrieval) + short provenance metadata (source, date, chunk_id)
    - On retrieval: rerank top-k, perform _entity extraction_ ‚Üí build mini-graph (local subgraph) ‚Üí summarize subgraph ‚Üí attach to prompt.
    - Use prompt caching for static system prompt + knowledge prefix.
2. **GraphRAG (deep relation reasoning)**
    
    - Precompute document-level entity graph (NER ‚Üí coref ‚Üí edges by cooccurrence/semantic relation).
    - Query: graph traversal from question entities ‚Üí collect nodes/paths ‚Üí transform to natural language evidence ‚Üí feed as RAG context.
3. **DSPy / Programmatic Agents (modular pipelines)**
    
    - Define agent behaviors as modules (e.g., retriever, verifier, planner, executor).
    - Use optimizers to tune the few-shot examples and module parameters automatically.

---

### 4.2 Prompt management & caching strategy (praktyka)

- **System prompt versioning:** numer wersji w cache key; przy aktualizacji system_prompta ‚Üí cache miss (forced rewrite).
- **Cache policy:** cache only >N token prefixes (e.g. >1024 tokens), evict by LRU or TTL (accounting for data staleness).

---

### 4.3 Format wymiany danych

- **Input (czytelno≈õƒá):** Markdown dla dokumentacji + XML delimiters dla komponent√≥w (Anthropic style).
- **Output (maszynowy):** JSON Schema / `response_format` (narzƒôdzie) lub token-oszczƒôdne TSV/CSV/TOON dla du≈ºych struktur. Testuj tokeny i compliance.

---

## 5. Implementacje (pseudokod ‚Äî trzy warianty agenta AI)

> Wszystkie przyk≈Çady poni≈ºej to **pseudokod**, celowo bez zale≈ºno≈õci bibliotecznych ‚Äî ≈Çatwo je zaadaptujesz do DSPy, LangChain, LlamaIndex, itp.

### 5.1 Wariant A ‚Äî _Simple RAG Agent + Prompt Caching_

```
AGENT_SIMPLE_RAG(question):
    # 1. Normalize question
    q_norm = normalize(question)

    # 2. If cache.has(system_prompt_version + q_norm.prefix):
    #       use cached_context = cache.get(...)
    #    else:
    #       base_context = system_prompt + static_knowledge_prefix
    #       cache.write(key=system_prompt_version + prefix, value=base_context)
    context_prefix = cache.fetch_or_write(system_prompt_version, static_knowledge_prefix)

    # 3. Retrieve top_k docs from vector_db using q_norm
    docs = vector_db.search(q_norm, k=10)

    # 4. Rerank / extract best n fragments
    fragments = rerank_and_select(docs, n=5)

    # 5. Build final prompt: context_prefix + fragments + instruction_footer
    final_prompt = concat(context_prefix, format_fragments(fragments), instruction_footer)

    # 6. Call LLM with prompt caching enabled
    answer = LLM.call(final_prompt, prompt_caching=True)

    # 7. Post-process: validate JSON schema or run 'LLM-as-judge' to score faithfulness
    if not validate_schema(answer):
        answer = run_llm_fixup(answer, final_prompt)
    return answer
```

**Gdzie ten wariant siƒô przydaje:** systemy Q&A, helpdesk, chatboty produktowe z czƒôsto-powtarzalnym kontekstem.

---

### 5.2 Wariant B ‚Äî _GraphRAG Agent (dla pyta≈Ñ relacyjnych / narracyjnych)_

```
AGENT_GRAPHRAG(question):
    q_entities = entity_extract(question)

    # 1. Traverse precomputed knowledge_graph from q_entities
    subgraph = graph.traverse(start_nodes=q_entities, depth=2, policy='relevance')

    # 2. Convert subgraph to linear evidence (summaries per path)
    evidence_snippets = []
    for path in subgraph.top_paths(limit=10):
        evidence_snippets.append(summarize_path(path))

    # 3. Retrieve supporting docs from vector_db for each node (optional)
    supporting_docs = vector_db.batch_search(nodes=subgraph.nodes)

    # 4. Compose prompt:
    #   [system_prompt_version + static_prefix] + [evidence_snippets] + [supporting_docs meta] + [instruction]
    prompt = compose_prompt(system_prefix, evidence_snippets, supporting_docs, question)

    # 5. Call LLM (no CoT step inserted; let model produce reasoning if needed)
    raw_answer = LLM.call(prompt)

    # 6. Use verifier model to check faithfulness to evidence_snippets
    score = verifier.score(raw_answer, evidence_snippets)
    if score < threshold:
        return fallback_to_human_review(raw_answer, score)
    else:
        return raw_answer
```

**Gdzie siƒô przydaje:** komplexowe analizy, wk≈Çady badawcze, compliance, due diligence.

---

### 5.3 Wariant C ‚Äî _Programmed Agent (DSPy style) z optimizerem prompt√≥w_

```
# Pseudokod modu≈Çowy ‚Äî moduly: Retriever, Planner, Executor, Verifier
AGENT_PROGRAMMED(question):
    plan = Planner.plan(question)      # planner returns ordered subtasks
    for subtask in plan:
        module = dispatch_module(subtask.type)
        # module has own prompt template + local examples
        response = module.run(subtask.payload)
        collect_responses(response)

    final_output = Executor.aggregate(collect_responses)
    verified = Verifier.verify(final_output, confidence_threshold)
    if not verified:
        # automatic optimizer: tune few-shot examples or prompt weights for failing subtasks
        Optimizer.tune(module.prompts where failed)
        # re-run only failed modules (cheap)
        re_run_failed_modules()
    return final_output
```

**G≈Ç√≥wna zaleta:** skalowalno≈õƒá, automatyczna optymalizacja prompt√≥w i modularny retry-logic.

---

## 6. Ewaluacja i metryki ‚Äî jak mierzyƒá, ≈ºeby wiedzieƒá, ≈ºe dzia≈Ça

- **Faithfulness / Hallucination rate** ‚Äî u≈ºyj LLM-as-judge (Ragas, DeepEval) + warstwy heurystycznej (exact match, grounded citations).
- **Token cost / Latency** ‚Äî instrumentuj: _input tokens_, _output tokens_, _p95 latency_, _cache hit rate_. Por√≥wnuj wersje A/B.
- **End-to-end success** ‚Äî task-specific metric (F1, accuracy, business KPI).
- **Robustness** ‚Äî testy adversarial prompt injection + tests for `lost-in-the-middle` (pozycja informacji).

---

## 7. Najwa≈ºniejsze poprawki do Twojego raportu (konkretne)

1. **Rozszerzyƒá rozdzia≈Ç o prompt caching** o: r√≥≈ºnice cenowe miƒôdzy dostawcami (cache write cost vs read cost), polityki wersjonowania, polityki prywatno≈õci. Dodaj diagram klucza cache (system_prompt_version + kb_hash).
2. **Doprecyzowaƒá procenty oszczƒôdno≈õci dla Markdown vs JSON** ‚Äî zaproponowaƒá eksperymenty i warto≈õci referencyjne zamiast sta≈Çego ‚Äû30‚Äì40%‚Äù.
3. **Uzupe≈Çniƒá RAG o hybrydowe patterny** (vector + graph + on-the-fly summarization) i por√≥wnaƒá koszty.
4. **Dodaƒá procedury testowe** (A/B dla CoT vs non-CoT; cache hit/miss tests; lost-in-the-middle placement experiments).

---

## 8. Przyk≈Çadowy szablon ¬´Super-Prompt¬ª (XML + Sandwich + JSON output)

> Kr√≥tkie, bo to ma byƒá wklejone jako system prompt; zachowaj oddzielne pliki dla d≈Çugich kontekst√≥w (cache'owane).

```
<system>
<version>v1.2</version>
<role>System: Expert AI Agent. Produce JSON matching schema below. Do NOT hallucinate.</role>
<constraints>
- Max tokens for answer: 800
- Cite only sources present in <evidence> blocks
- If uncertain, return {"needs_human_review": true, "reason": "..."} 
</constraints>
</system>

<evidence>
# (this block should be filled from RAG retrieval; keep size <= 3000 tokens)
- source_id: DOC_2025_001
- excerpt: "..."
</evidence>

<task>
User question: {{QUESTION}}
Instruction: Answer in JSON matching schema: { "answer": string, "sources": [{"id":..., "span":...}], "confidence": 0-100 }
End instruction ‚Äî repeat: Return valid JSON only.
</task>
```

---

## 9. Bibliografia i ≈∫r√≥d≈Ça (wybrane, uporzƒÖdkowane ‚Äî kluczowe i dodatkowe)

> Poni≈ºej podajƒô najwa≈ºniejsze ≈∫r√≥d≈Ça u≈ºyte przy weryfikacji ‚Äî kliknij, ≈ºeby otworzyƒá.

**Kluczowe (5 najbardziej ‚Äûload-bearing‚Äù)**

- Anthropic ‚Äî Prompt engineering, XML tags (Claude docs).
- OpenAI ‚Äî Prompt Caching (official docs / cookbook).
- Microsoft Research ‚Äî GraphRAG project (GraphRAG: Unlocking LLM discovery on narrative private data).
- DSPy ‚Äî Framework ¬´programming, not prompting¬ª. (repo / docs).
- Nelson F. Liu et al. ‚Äî _Lost in the Middle: How Language Models Use Long Contexts_ (paper + repo).

**Dodatkowe (przydatne do implementacji i praktycznych test√≥w)**

- AWS Bedrock ‚Äî Prompt caching docs.
- Azure OpenAI ‚Äî Prompt caching guide.
- Medium / Case studies on prompt caching (por√≥wnania ekonomiczne, praktyki).
- Studies and experiments on Markdown vs JSON token usage; TOON and token-oriented formats.
- Ragas / evaluation frameworks (GraphRAG/Eval literature).

> Pe≈Çne listy link√≥w (bez ‚Äûraw‚Äù url w tek≈õcie): cytowania powy≈ºej zawierajƒÖ odno≈õniki do wynik√≥w wyszukiwania ‚Äî kliknij powiƒÖzania w narzƒôdziu konwersacji, ≈ºeby otworzyƒá ka≈ºdƒÖ pozycjƒô.

---

## 10. Za≈ÇƒÖcznik: Konkretne testy do uruchomienia (quick checklist)

1. **Prompt caching smoke test**
    
    - Mierzyƒá tokeny i latencjƒô: 10 powt√≥rze≈Ñ z identycznym prefiksem (cache hit) i z innym prefiksem (miss). Por√≥wnaƒá koszty.
2. **Markdown vs JSON A/B**
    
    - We≈∫ ten sam dokument jako Markdown, JSON, TSV; policz input tokens i sprawd≈∫ accuracy odpowiedzi. Raportuj delta token√≥w.
3. **Lost-in-the-middle placement experiments**
    
    - Umie≈õƒá kluczowe informacje (A) na poczƒÖtku, (B) w ≈õrodku, (C) na ko≈Ñcu i por√≥wnaj recall.
4. **GraphRAG value test**
    
    - Dla zestawu 100 pyta≈Ñ ‚Äûglobalnych‚Äù (wymagajƒÖcych relacji) por√≥wnaj Vector RAG vs GraphRAG (precision/recall + human eval).

---

## 11. Pe≈Çna (rozszerzona) wersja raportu ‚Äî co mogƒô wygenerowaƒá teraz

W za≈ÇƒÖczeniu powy≈ºej skondensowa≈Çem rewizjƒô i doda≈Çem pseudokod, testy i bibliografiƒô. Je≈õli chcesz, mogƒô teraz (bez pyta≈Ñ dodatkowych):

- wygenerowaƒá **pe≈Çny, rozbudowany dokument Markdown** (gotowy do druku/CRM) z diagramami architektury (w postaci ASCII / opis√≥w), lub
- wypisaƒá **szczeg√≥≈Çowy checklist CI/CD** do automatycznego benchmarkingu prompt√≥w i cache, lub
- wygenerowaƒá **rozszerzony pseudokod** dla wybranego wariantu agenta (A/B/C) z bardziej szczeg√≥≈Çowymi procedurami retry, wersjonowania cache i walidacji.

---

Jasne ‚Äî poni≈ºej masz **kompletny, pe≈Çny, scalony raport**, zawierajƒÖcy **wszystkie proponowane uzupe≈Çnienia**, pe≈ÇnƒÖ strukturƒô, pseudokod, rekomendacje, checklisty i bibliografiƒô.  
Wszystko w jednym pliku **Markdown**, gotowe do eksportu lub zapisania jako `.md`.

---

# **üìò Kompleksowy Raport Optymalizacji i Implementacji Zaawansowanych Agent√≥w AI (Wersja Rozszerzona)**

_Pe≈Çna analiza + poprawiona tre≈õƒá + dodane rekomendacje + pe≈Çny pseudokod + bibliografia_

---

# **1. Executive Summary**

Ten dokument przedstawia:

- **Weryfikacjƒô i aktualizacjƒô** tez z pierwotnego raportu dotyczƒÖcego pracy z nowoczesnymi LLM.
- **Rozszerzenie** o nowe, potwierdzone praktyki: GraphRAG, DSPy, prompt caching, testy A/B, metryki, schematy pipeline'√≥w.
- **Warianty implementacji agent√≥w** (RAG, GraphRAG, DSPy).
- **Pseudokod** wszystkich modu≈Ç√≥w.
- **RozszerzonƒÖ bibliografiƒô** i checklisty CI/CD.

---

# **2. Walidacja i korekta pierwotnych twierdze≈Ñ**

## **2.1 XML Tags / Strukturyzowane prompty**

**Status:** Potwierdzone i aktualne.

- Anthropic oficjalnie rekomenduje u≈ºywanie XML do separacji instrukcji od danych.
- XML minimalizuje ryzyko injection, poprawia stabilno≈õƒá i czytelno≈õƒá.

**Uzupe≈Çnienie:**  
Dodaƒá warstwƒô sanitacji wej≈õcia oraz walidacjƒô metadanych (source/date/author).

---

## **2.2 Prompt Caching**

**Status:** Potwierdzone, wymaga doprecyzowania.

- OpenAI/Anthropic wspierajƒÖ caching du≈ºych prefiks√≥w.
- Oszczƒôdno≈õci mogƒÖ wynosiƒá od **50% do ponad 90%** w zale≈ºno≈õci od modelu i sposobu cacheowania (write/read cost).

**Uzupe≈Çnienie:**  
Cache musi byƒá wersjonowany (`system_prompt_version`, `kb_hash`).  
Przy intensywnej rotacji danych efektywno≈õƒá maleje.

---

## **2.3 Markdown vs JSON**

**Status:** Og√≥lnie tak, szczeg√≥≈Çy wymagajƒÖ korekty.

- Markdown jest bardziej token-efektywny, ale przewaga waha siƒô **11‚Äì30%**, zale≈ºnie od struktury.
- JSON jest zalecany jako output, bo jest ≈Çatwo parsowalny.
- dla du≈ºych struktur ‚Üí CSV/TSV/TOON mogƒÖ byƒá bardziej ekonomiczne.

---

## **2.4 Chain-of-Thought i reasoning models**

**Status:** Czƒô≈õciowo prawda.

- Nowe modele reasoning (np. o1, o3) generujƒÖ wewnƒôtrzny reasoning i nie zawsze wymagajƒÖ jawnego CoT.
- Ale CoT nie jest przestarza≈Çy ‚Äî nadal przydatny w wielu zadaniach logicznych.

**Rekomendacja:**  
Testowaƒá A/B ‚Üí _CoT vs No-CoT_ ‚Üí mierzyƒá accuracy + token cost + latency.

---

## **2.5 GraphRAG**

**Status:** Potwierdzone i aktualne.

- GraphRAG znaczƒÖco przewy≈ºsza Vector RAG w pytaniach wymagajƒÖcych relacji, przyczynowo≈õci, powiƒÖza≈Ñ oraz rozsypanych danych.
- Wymaga ciƒô≈ºszej fazy preprocesingu i prowadzenia grafu wiedzy.

---

# **3. Aktualne najlepsze praktyki (2025)**

## **3.1 Architektura hybrydowa RAG**

Najskuteczniejszy wsp√≥≈Çczesny wzorzec to:

> **VectorDB + Knowledge Graph + Local Subgraph Summaries + Prompt Caching**

Pozwala uzyskaƒá:

- precyzyjne odpowiedzi (vector retrieval),
- rozumienie relacyjne (graph traversal),
- niskie koszty (caching prefiks√≥w).

---

## **3.2 DSPy (programming-not-prompting)**

- Modularne i optymalizowalne prompty.
- Logika podzielona na: _Planner_, _Retriever_, _Executor_, _Verifier_, _Optimizer_.

---

## **3.3 Lost-in-the-Middle mitigation**

- dziel d≈Çugie dokumenty na **tematyczne** chunk'i (nie arbitralne tokeny),
- najwa≈ºniejsze elementy zawsze na poczƒÖtku i ko≈Ñcu kontekstu,
- stosuj **entity recall blocks**.

---

## **3.4 Prompt Management**

### 3.4.1 Wersjonowanie

```
cache_key = hash(system_prompt_version + kb_version + user_prompt_prefix)
```

### 3.4.2 Polityka wygaszania

- TTL 1‚Äì7 dni dla statycznej wiedzy
- LRU dla dynamicznych agent√≥w

---

# **4. Trzy kompletne architektury agent√≥w (pe≈Çne pseudokody)**

---

# **4.1 Agent A ‚Äî Simple RAG + Prompt Caching**

```
AGENT_SIMPLE_RAG(question):
    q_norm = normalize(question)

    context_prefix = cache.fetch_or_write(
         key = hash(system_prompt_version + static_kb_hash),
         value = system_prompt + static_kb
    )

    docs = vector_db.search(q_norm, k=10)
    fragments = rerank_and_select(docs, top=5)

    final_prompt = concat(
         context_prefix,
         format_fragments(fragments),
         build_instruction_footer(question)
    )

    answer = LLM.call(final_prompt, prompt_caching=True)

    if not validate_schema(answer):
        answer = fix_with_llm(answer, final_prompt)

    return answer
```

**Zastosowanie:** chatboty produktowe, dokumentacja, helpdesk.

---

# **4.2 Agent B ‚Äî GraphRAG Agent (dla analiz relacyjnych)**

```
AGENT_GRAPHRAG(question):
    q_entities = extract_entities(question)

    subgraph = graph.traverse(
        start_nodes=q_entities,
        depth=2,
        policy='semantic_relevance'
    )

    evidence_snippets = []
    for path in subgraph.top_paths(limit=12):
        evidence_snippets.append(summarize_path(path))

    supporting_docs = vector_db.batch_search(subgraph.nodes)

    prompt = compose(
        system_prefix,
        evidence_snippets,
        supporting_docs,
        question
    )

    raw_answer = LLM.call(prompt)

    score = verifier.score(raw_answer, evidence_snippets)

    if score < threshold:
        return escalate_to_human(raw_answer)

    return raw_answer
```

**Zastosowanie:** analizy prawne, dziennikarskie, due diligence, compliance.

---

# **4.3 Agent C ‚Äî Programmed Agent (DSPy-style)**

```
AGENT_PROGRAMMED(question):
    plan = Planner.plan(question)

    results = []

    for subtask in plan:
        module = dispatch(subtask.type)
        result = module.run(subtask)
        results.append(result)

    final_output = Executor.aggregate(results)

    if not Verifier.verify(final_output):
        Optimizer.adjust_prompts(for_failed_modules)
        re_run_failed_modules()

    return final_output
```

**Zastosowanie:** systemy zadaniowe, multi-step workflows, asystenci klasy enterprise.

---

# **5. Testy, metryki i CI/CD dla agent√≥w AI**

---

## **5.1 Metryki jako≈õci**

|Metryka|Opis|
|---|---|
|**Faithfulness**|zgodno≈õƒá z kontekstem (LLM-as-judge + heurystyki)|
|**Hallucination Rate**|odsetek odpowiedzi nieopartych o evidence|
|**Latency (p95)**|kluczowe przy wielu submodu≈Çach|
|**Token Cost**|input, output, cache hit ratio|
|**End-to-End Success**|F1/accuracy/KPI biznesowe|

---

## **5.2 Testy obowiƒÖzkowe**

### **A/B CoT vs No-CoT**

- Zbadaƒá accuracy + tokeny + czas.

### **Lost-in-the-middle**

- sprawdziƒá placement kluczowych danych: start vs ≈õrodek vs koniec.

### **Prompt Caching (prefiks > 1500 token√≥w)**

- 10√ó zapyta≈Ñ:
    - identyczny prefiks ‚Üí powinien byƒá cache hit
    - zmieniony ‚Üí miss

### **GraphRAG value test**

- 100 pyta≈Ñ relacyjnych ‚Äî por√≥wnaƒá RAG vs GraphRAG.

---

# **6. Super-Prompt Template (XML + Sandbox + JSON Output)**

```
<system>
<version>v1.2</version>
<role>
You are an expert AI system. 
Your output MUST be valid JSON. 
Do not invent facts. 
Cite only evidence included below.
</role>

<constraints>
- Max 800 tokens
- If unsure ‚Üí {"needs_human_review": true}
</constraints>
</system>

<evidence>
# Inserted by retrieval module
# Max 3000 tokens
</evidence>

<task>
User question: {{QUESTION}}

Return valid JSON:
{
  "answer": "...",
  "sources": [{"id":"...", "span":"..."}],
  "confidence": 0-100
}
</task>
```

---

# **7. Pe≈Çna bibliografia (z linkami dziƒôki cytowaniom w konwersacji)**

> Ka≈ºde ≈∫r√≥d≈Ço otworzysz klikajƒÖc identyfikator cytowania.

### **Najwa≈ºniejsze (core)**

- **Anthropic ‚Äî XML & structured prompting**
    
- **OpenAI ‚Äî Prompt caching (cookbook + docs)**
    
- **Microsoft Research ‚Äî GraphRAG**
    
- **DSPy ‚Äî programmatic framework**
    
- **Lost in the Middle ‚Äî Nelson F. Liu et al.**
    

### **Token efficiency / formats**

- Markdown vs JSON token efficiency

### **Prompt caching case studies**

- Medium / praktyczne case‚Äôy
    
- AWS Bedrock prompt caching
    
- Azure caching
    

### **Ewaluacja / testy RAG**

- RAGAS, GraphRAG evaluation

---

# **8. Checklist ‚Äî wymagania do wdro≈ºenia produkcyjnego**

### **8.1 Prompt Management**

- [ ] system prompt versioning
- [ ] cache key = hash(system_prompt_version + kb_version)
- [ ] retry logic + fix-up prompts
- [ ] XML/Markdown delimiters

### **8.2 RAG/GraphRAG**

- [ ] chunking tematyczne
- [ ] reranking przed kontekstem
- [ ] evidence numbering
- [ ] graph summarization pipeline

### **8.3 Monitoring**

- [ ] token logs
- [ ] latency p95
- [ ] hallucination detector
- [ ] cache hit/miss

### **8.4 CI/CD**

- [ ] A/B test CoT
- [ ] Lost-in-the-middle test
- [ ] GraphRAG vs VectorRAG comparison
- [ ] JSON schema validation

---
